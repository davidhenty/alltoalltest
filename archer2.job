#!/bin/bash

# Slurm job options (name, compute nodes, job time)
#SBATCH --job-name=a2atest
#SBATCH --time=00:20:00
#SBATCH --output=%x-%j.out
#SBATCH --nodes=8
#SBATCH --tasks-per-node=128
#SBATCH --cpus-per-task=1
# Replace [budget code] below with your budget code (e.g. t01)
#SBATCH --account=z19
#SBATCH --qos=short
#SBATCH --partition=standard
#SBATCH --reservation=shortqos

echo "+------------------------------------------------------+"
echo "| Start of contents of SLURM job script for job $SLURM_JOB_ID |"
echo "+------------------------------------------------------+"

cat $0

echo "+------------------------------------------------------+"
echo "|  End of contents of SLURM job script for job $SLURM_JOB_ID  |"
echo "+------------------------------------------------------+"
echo

module --silent load epcc-job-env

module swap cray-mpich cray-mpich-ucx
module swap craype-network-ofi craype-network-ucx
#export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/opt/cray/pe/mpich/8.0.16/ucx/cray/9.1/lib/pkgconfig/

 # Set the number of threads to 1
#   This prevents any threaded system libraries from automatically
#   using threading.

export OMP_NUM_THREADS=1

srun --unbuffered --distribution=block:block --hint=nomultithread \
     ./alltoalltest
